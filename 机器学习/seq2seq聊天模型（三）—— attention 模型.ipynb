{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力seq2seq模型\n",
    "大部分的seq2seq模型，对所有的输入，一视同仁，同等处理。\n",
    "\n",
    "但实际上，输出是由输入的各个重点部分产生的。\n",
    "\n",
    "比如：\n",
    "<img width=300 src='./截图/attention.jpg'>\n",
    "\n",
    "\n",
    "\n",
    "（举例使用，实际比重不是这样）\n",
    "对于输出“晚上”，\n",
    "\n",
    "各个输入所占比重:  今天-50%，晚上-50%，吃-100%，什么-0%\n",
    "\n",
    "\n",
    "对于输出“吃”，\n",
    "\n",
    "各个输入所占比重:  今天-0%，晚上-0%，吃-100%，什么-0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别是在seq2seq的看图说话应用情景中\n",
    "<img width=300 src='./截图/ranran.jpg'>\n",
    "睡觉还握着笔的baby\n",
    "\n",
    "这里的重点就是baby，笔！通过这些重点，生成描述。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面这个图，就是attention的关键原理 \n",
    "<img src='./截图/attentionbasic.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorlfow 代码\n",
    "encoder 和常规的seq2seq中的encoder一样，只是在attention模型中，不再需要encoder累计的state状态，需要的是各个各个分词的outputs输出。\n",
    "\n",
    "在训练的时候，将这个outputs与一个权重值一起拟合逼进目标值。\n",
    "\n",
    "这个权重值，就是各个输入对目标值的贡献占比，也就是注意力机制！\n",
    "\n",
    "```\n",
    "dec_cell = self.cell(self.hidden_size)\n",
    "\n",
    "attn_mech = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=self.attn_size,  # 注意机制权重的size\n",
    "    memory=self.enc_outputs,  # 主体的记忆，就是decoder输出outputs\n",
    "    memory_sequence_length=self.enc_sequence_length,\n",
    "    #   normalize=False,\n",
    "    name='LuongAttention')\n",
    "\n",
    "dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    cell=dec_cell,\n",
    "    attention_mechanism=attn_mech,\n",
    "    attention_layer_size=self.attn_size,\n",
    "    #  attention_history=False, # (in ver 1.2)\n",
    "    name='Attention_Wrapper')\n",
    "initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "\n",
    "# output projection (replacing `OutputProjectionWrapper`)\n",
    "output_layer = Dense(dec_vocab_size + 2, name='output_projection')\n",
    "\n",
    "\n",
    " # lstm的隐藏层size和attention 注意机制权重的size要相同\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
