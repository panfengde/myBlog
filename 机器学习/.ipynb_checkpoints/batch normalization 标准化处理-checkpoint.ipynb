{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 该问题的出现是因为我们没有同等程度的看待各个特征，即我们没有将各个特征量化到统一的区间。\n",
    "\n",
    "     数据标准化（归一化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。以下是两种常用的归一化方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    " Standardization又称为Z-score normalization，量化后的特征将服从标准正态分布：\n",
    " \n",
    " $$\n",
    " z=\\frac{x_i-\\mu}{\\sigma}\n",
    " $$\n",
    "其中，u和sigma分别为对应特征的均值和标准差。量化后的特征将分布在[-1, 1]区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "标准差=\\sigma=\\sqrt{\\frac{1}{N}\\sum_{i=0}^{N}{(x_i-\\mu)^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "样本方差=\\sigma^2\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Scaling\n",
    " Min-Max Scaling又称为Min-Max normalization， 特征量化的公式为：\n",
    "\n",
    "$$\n",
    " z=\\frac{x_i-min(x_i)}{max(x_i)-min()x_i}\n",
    " $$\n",
    " 大多数机器学习算法中，会选择Standardization来进行特征缩放，但是，Min-Max Scaling也并非会被弃置一地,量化后的特征将分布在[0, 1]区间。在数字图像处理中，像素强度通常就会被量化到[0,1]区间，在一般的神经网络算法中，也会要求特征被量化[0，1]区间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN 添加位置\n",
    "全连接等后面，激活函数前面\n",
    "<img src='./截图/BN.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN 效果----修改\n",
    "Batch normalization 也可以被看做一个层面. 在一层层的添加神经网络的时候, 我们先有数据 X, 再添加全连接层, 全连接层的计算结果会经过 激励函数 成为下一层的输入, 接着重复之前的操作. Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间.\n",
    "\n",
    "\n",
    "之前说过, 计算结果在进入激励函数前的值很重要, 如果我们不单单看一个值, 我们可以说, 计算结果值的分布对于激励函数很重要. 对于数据值大多分布在这个区间的数据, 才能进行更有效的传递. 对比这两个在激活之前的值的分布. 上者没有进行 normalization, 下者进行了 normalization, 这样当然是下者能够更有效地利用 tanh 进行非线性化的过程.\n",
    "\n",
    "\n",
    "没有 normalize 的数据 使用 tanh 激活以后, 激活值大部分都分布到了饱和阶段, 也就是大部分的激活值不是-1, 就是1, 而 normalize 以后, 大部分的激活值在每个分布区间都还有存在. 再将这个激活后的分布传递到下一层神经网络进行后续计算, 每个区间都有分布的这一种对于神经网络就会更加有价值. Batch normalization 不仅仅 normalize 了一下数据, 他还进行了反 normalize 的手续. 为什么要这样呢?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
