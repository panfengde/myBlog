{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在做seq2seq的时候，经常需要使用sequence_loss这是损失函数。\n",
    "\n",
    "\n",
    "####  <font color=#FF0000 >原创文章，转载请注明出处 </font>\n",
    "   \n",
    "现在分析一下sequence_loss这个函数到底在做什么\n",
    "\n",
    "```\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.seq2seq import sequence_loss\n",
    "\n",
    "logits_np = np.array([\n",
    "   [[1.0, 2.0], [1.0, 2.0]],\n",
    "    [[1.0, 2.0], [1.0, 2.0]]\n",
    "])\n",
    "\n",
    "targets_np = np.array([\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "], dtype=np.int32)\n",
    "\n",
    "logits = tf.convert_to_tensor(logits_np)\n",
    "targets = tf.convert_to_tensor(targets_np)\n",
    "cost = sequence_loss(logits=logits,\n",
    "                     targets=targets,\n",
    "                     weights=tf.ones_like(targets, dtype=tf.float64))\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    r = sess.run(cost)\n",
    "    print(r)\n",
    "    \n",
    "# sequence_loss的结果是0.563261687518082\n",
    "\n",
    "```\n",
    "\n",
    "求loss值\n",
    "$$\n",
    "logits=\\left[\\begin{matrix} [1.0, 2.0] & [1.0, 2.0] \\cr [1.0, 2.0] & [1.0, 2.0]\\end{matrix}\\right]\n",
    "$$\n",
    "$$\n",
    "target=\\left[\\begin{matrix} 0.0 & 1.0 \\cr 1.0 & 1.0 \\end{matrix}\\right]\n",
    "$$\n",
    "$$\n",
    "cost=sequence\\_loss( logits=logits,targets=targets,weights=tf.ones_like(targets, dtype=tf.float64))\n",
    "$$\n",
    "\n",
    "\n",
    "### sequence_loss的求值过程\n",
    "1.softmax求值 \n",
    "\n",
    "2.交叉熵选择\n",
    "\n",
    "3.求平均值\n",
    "\n",
    "\n",
    "\n",
    "### 1.softmax\n",
    "\n",
    "将得分或者概率fi，统一转化到0到1之间，就是计算权重占比（归一化处理）\n",
    "但是在计算权重的时候，分数都通过自然数e映射转换，<font color=#FF0000 >目的是，让大的分数更大，让小的分数更小，增加区分度</font>\n",
    "$$f_i(z)=-log( \\frac{ e^{f_i} }{ \\sum{e^{f_j} }} )$$\n",
    "\n",
    "其输入值是一个向量，向量中元素为任意实数的得分值\n",
    "\n",
    "输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1（计算每个得分在总分中的占比。<font color=#FF0000 >这里通过指数映射了一下 </font>）\n",
    "\n",
    "\n",
    "$$f_i(z)=-log( \\frac{ e^{f_i} }{ \\sum{e^{f_j} }} )$$\n",
    "\n",
    "\n",
    "logits = [\n",
    "    [[1.0, 2.0], [1.0, 2.0]],\n",
    "    [[1.0, 2.0], [1.0, 2.0]]\n",
    "]\n",
    "\n",
    "$$\n",
    "softmax=\\left[\\begin{matrix} [ \\frac{ e^{1.0} }{ e^{1.0}+e^{2.0}}  ,  \\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}] & [\\frac{ e^{1.0} }{ e^{1.0}+e^{2.0}},\\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}] \\cr [\\frac{ e^{1.0} }{ e^{1.0}+e^{2.0}},\\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}] & [\\frac{ e^{1.0} }{ e^{1.0}+e^{2.0}},\\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}]\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### 2求交叉熵\n",
    "\n",
    "\n",
    "targets = [\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "    ]\n",
    "根据targets, 确定选取哪个值。\n",
    "\n",
    "\n",
    "$$\n",
    "crross\\_softmax=\\left[\\begin{matrix} -log(\\frac{ e^{1.0} }{ e^{1.0}+e^{2.0}}) & -log(\\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}) \\cr -log(\\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}) & -log(\\frac{ e^{2.0} }{ e^{1.0}+e^{2.0}}\\end{matrix}) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "```\n",
    "再求平均值\n",
    "loss1=(-log(2.718/(2.718+7.387))+(-log(7.387/(2.718+7.387))))/2\n",
    "loss2=(-log(7.387/(2.718+7.387))+(-log(7.387/(2.718+7.387))))/2\n",
    "loss=(loss1+loss2)/2\n",
    "loss=0.563\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
